{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6b1f4d",
   "metadata": {},
   "source": [
    "# DINO Grounding Object Detection for VLSP 2025 Traffic Sign Dataset\n",
    "\n",
    "This notebook performs object detection on all training images using the DINO Grounding model to detect traffic signs.\n",
    "The detected objects will be annotated and saved to a new folder for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82245df",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222c7403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhieu/anaconda3/envs/vlsp25/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90939417",
   "metadata": {},
   "source": [
    "## Setup Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee975d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "BASE_PATH = Path(\"/home/mhieu/git/vlsp25\")\n",
    "TRAIN_IMAGES_PATH = BASE_PATH / \"dataset/vlsp25/private_test/private_test_images\"\n",
    "OUTPUT_PATH = BASE_PATH / \"image\" / \"private_detected_objects\"\n",
    "ANNOTATED_IMAGES_PATH = OUTPUT_PATH / \"annotated_images\"\n",
    "CROPPED_IMAGES_PATH = OUTPUT_PATH / \"cropped_objects\"\n",
    "DETECTION_RESULTS_PATH = OUTPUT_PATH / \"detection_results\"\n",
    "\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(ANNOTATED_IMAGES_PATH, exist_ok=True)\n",
    "os.makedirs(CROPPED_IMAGES_PATH, exist_ok=True)\n",
    "os.makedirs(DETECTION_RESULTS_PATH, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f772a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train images path: /home/mhieu/git/vlsp25/dataset/vlsp25/private_test/private_test_images\n",
      "Output path: /home/mhieu/git/vlsp25/image/private_detected_objects\n",
      "Annotated images will be saved to: /home/mhieu/git/vlsp25/image/private_detected_objects/annotated_images\n",
      "Cropped objects will be saved to: /home/mhieu/git/vlsp25/image/private_detected_objects/cropped_objects\n"
     ]
    }
   ],
   "source": [
    "# # Define paths\n",
    "# BASE_PATH = Path(\"/home/mhieu/git/vlsp25\")\n",
    "# TRAIN_IMAGES_PATH = BASE_PATH / \"dataset/VLSP 2025 - MLQA-TSR Data Release/train_data/train_images\"\n",
    "# OUTPUT_PATH = BASE_PATH / \"detected_objects\"\n",
    "# ANNOTATED_IMAGES_PATH = OUTPUT_PATH / \"annotated_images\"\n",
    "# CROPPED_IMAGES_PATH = OUTPUT_PATH / \"cropped_objects\"\n",
    "# DETECTION_RESULTS_PATH = OUTPUT_PATH / \"detection_results\"\n",
    "\n",
    "# # Create output directories\n",
    "# os.makedirs(ANNOTATED_IMAGES_PATH, exist_ok=True)\n",
    "# os.makedirs(CROPPED_IMAGES_PATH, exist_ok=True)\n",
    "# os.makedirs(DETECTION_RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = \"IDEA-Research/grounding-dino-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEXT_PROMPT = \"traffic sign.\"\n",
    "BOX_THRESHOLD = 0.2\n",
    "TEXT_THRESHOLD = 0.2\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Train images path: {TRAIN_IMAGES_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")\n",
    "print(f\"Annotated images will be saved to: {ANNOTATED_IMAGES_PATH}\")\n",
    "print(f\"Cropped objects will be saved to: {CROPPED_IMAGES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89932ad8",
   "metadata": {},
   "source": [
    "## Load DINO Grounding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4a335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINO Grounding model...\n",
      "Model loaded successfully!\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading DINO Grounding model...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d052f37",
   "metadata": {},
   "source": [
    "## Get List of Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8a1cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 104 training images\n",
      "First 5 images: ['private_test_1_1.jpg', 'private_test_1_2.jpg', 'private_test_1_3.jpg', 'private_test_1_4.jpg', 'private_test_1_5.jpg']\n",
      "Last 5 images: ['private_test_8_5.jpg', 'private_test_8_6.jpg', 'private_test_8_7.jpg', 'private_test_8_8.jpg', 'private_test_8_9.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Get all training images\n",
    "image_files = list(TRAIN_IMAGES_PATH.glob(\"*.jpg\")) + list(TRAIN_IMAGES_PATH.glob(\"*.png\"))\n",
    "image_files.sort()\n",
    "\n",
    "print(f\"Found {len(image_files)} training images\")\n",
    "print(f\"First 5 images: {[img.name for img in image_files[:5]]}\")\n",
    "print(f\"Last 5 images: {[img.name for img in image_files[-5:]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301fff0",
   "metadata": {},
   "source": [
    "## Define Object Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d42ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_in_image(image_path, text_prompt=TEXT_PROMPT, box_threshold=BOX_THRESHOLD, text_threshold=TEXT_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Detect objects in a single image using DINO Grounding model\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        text_prompt: Text prompt for object detection\n",
    "        box_threshold: Threshold for bounding box confidence\n",
    "        text_threshold: Threshold for text matching confidence\n",
    "    \n",
    "    Returns:\n",
    "        dict: Detection results with boxes, scores, labels, and image info\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and process image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Prepare inputs\n",
    "        inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Post-process results\n",
    "        results = processor.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            inputs.input_ids,\n",
    "            target_sizes=[image.size[::-1]]\n",
    "        )\n",
    "        \n",
    "        result = results[0]\n",
    "        \n",
    "        # Apply thresholds manually\n",
    "        if len(result[\"scores\"]) > 0:\n",
    "            # Filter based on score threshold (box_threshold)\n",
    "            score_mask = result[\"scores\"] >= box_threshold\n",
    "            filtered_boxes = result[\"boxes\"][score_mask]\n",
    "            filtered_scores = result[\"scores\"][score_mask]\n",
    "            filtered_labels = [label for i, label in enumerate(result[\"labels\"]) if score_mask[i]]\n",
    "            \n",
    "            result = {\n",
    "                \"boxes\": filtered_boxes,\n",
    "                \"scores\": filtered_scores,\n",
    "                \"labels\": filtered_labels\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"image_path\": str(image_path),\n",
    "            \"image_name\": image_path.name,\n",
    "            \"image_size\": image.size,\n",
    "            \"num_detections\": len(result[\"boxes\"]),\n",
    "            \"boxes\": result[\"boxes\"].tolist() if len(result[\"boxes\"]) > 0 else [],\n",
    "            \"scores\": result[\"scores\"].tolist() if len(result[\"scores\"]) > 0 else [],\n",
    "            \"labels\": result[\"labels\"] if len(result[\"labels\"]) > 0 else [],\n",
    "            \"text_prompt\": text_prompt,\n",
    "            \"box_threshold\": box_threshold,\n",
    "            \"text_threshold\": text_threshold\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return {\n",
    "            \"image_path\": str(image_path),\n",
    "            \"image_name\": image_path.name,\n",
    "            \"error\": str(e),\n",
    "            \"num_detections\": 0,\n",
    "            \"boxes\": [],\n",
    "            \"scores\": [],\n",
    "            \"labels\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4d9af",
   "metadata": {},
   "source": [
    "## Define Annotation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce36b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_and_save_image(image_path, detection_result, output_path):\n",
    "    \"\"\"\n",
    "    Annotate image with detection results and save to output path\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to original image\n",
    "        detection_result: Detection results from DINO model\n",
    "        output_path: Path to save annotated image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load original image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        # Try to load a font, fallback to default if not available\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "        except:\n",
    "            try:\n",
    "                font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 20)\n",
    "            except:\n",
    "                font = ImageFont.load_default()\n",
    "        \n",
    "        # Draw bounding boxes and labels\n",
    "        for box, score, label in zip(detection_result[\"boxes\"], detection_result[\"scores\"], detection_result[\"labels\"]):\n",
    "            # Convert box to list if it's not already\n",
    "            if not isinstance(box, list):\n",
    "                box = box.tolist()\n",
    "            \n",
    "            # Create label text with confidence score\n",
    "            label_text = f\"{label} ({score:.2f})\"\n",
    "            \n",
    "            # Draw bounding box\n",
    "            draw.rectangle(box, outline=\"red\", width=3)\n",
    "            \n",
    "            # Draw label\n",
    "            # draw.text((box[0], box[1] - 25), label_text, fill=\"red\", font=font)\n",
    "        \n",
    "        # Add summary text\n",
    "        # summary_text = f\"Detections: {detection_result['num_detections']}\"\n",
    "        # draw.text((10, 10), summary_text, fill=\"blue\", font=font)\n",
    "        \n",
    "        # Save annotated image\n",
    "        image.save(output_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error annotating {image_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3260c82",
   "metadata": {},
   "source": [
    "## Define Crop Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db18ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_detections(image_path, detection_result, output_dir):\n",
    "    \"\"\"\n",
    "    Crop detected objects from the image and save them as individual files\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to original image\n",
    "        detection_result: Detection results from DINO model\n",
    "        output_dir: Directory to save cropped images\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load original image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_name = Path(image_path).stem\n",
    "        \n",
    "        saved_crops = []\n",
    "        \n",
    "        # Crop each detected object\n",
    "        for i, (box, score, label) in enumerate(zip(detection_result[\"boxes\"], detection_result[\"scores\"], detection_result[\"labels\"])):\n",
    "            # Convert box to list if it's not already\n",
    "            if not isinstance(box, list):\n",
    "                box = box.tolist()\n",
    "            \n",
    "            # Ensure box coordinates are within image bounds\n",
    "            x1, y1, x2, y2 = box\n",
    "            x1 = max(0, int(x1))\n",
    "            y1 = max(0, int(y1))\n",
    "            x2 = min(image.width, int(x2))\n",
    "            y2 = min(image.height, int(y2))\n",
    "            \n",
    "            # Skip if box is too small or invalid\n",
    "            if x2 <= x1 or y2 <= y1 or (x2 - x1) < 10 or (y2 - y1) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Crop the image\n",
    "            cropped_image = image.crop((x1, y1, x2, y2))\n",
    "            \n",
    "            # Create filename for the cropped image\n",
    "            clean_label = label.replace(\" \", \"_\").replace(\".\", \"\")\n",
    "            crop_filename = f\"{image_name}_crop_{i+1}_{clean_label}_{score:.2f}.jpg\"\n",
    "            crop_path = output_dir / crop_filename\n",
    "            \n",
    "            # Save the cropped image\n",
    "            cropped_image.save(crop_path, quality=95)\n",
    "            \n",
    "            saved_crops.append({\n",
    "                \"crop_filename\": crop_filename,\n",
    "                \"crop_path\": str(crop_path),\n",
    "                \"box\": box,\n",
    "                \"score\": score,\n",
    "                \"label\": label,\n",
    "                \"crop_size\": cropped_image.size\n",
    "            })\n",
    "        \n",
    "        return saved_crops\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error cropping objects from {image_path}: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e997b",
   "metadata": {},
   "source": [
    "## Process All Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e041fdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting object detection on 104 images...\n",
      "Results will be saved to: /home/mhieu/git/vlsp25/image/private_detected_objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/104 [00:00<?, ?it/s]/home/mhieu/anaconda3/envs/vlsp25/lib/python3.11/site-packages/transformers/models/grounding_dino/processing_grounding_dino.py:94: FutureWarning: The key `labels` is will return integer ids in `GroundingDinoProcessor.post_process_grounded_object_detection` output since v4.51.0. Use `text_labels` instead to retrieve string object names.\n",
      "  warnings.warn(self.message, FutureWarning)\n",
      "Processing images:   1%|          | 1/104 [00:03<06:37,  3.86s/it]/home/mhieu/anaconda3/envs/vlsp25/lib/python3.11/site-packages/transformers/models/grounding_dino/processing_grounding_dino.py:94: FutureWarning: The key `labels` is will return integer ids in `GroundingDinoProcessor.post_process_grounded_object_detection` output since v4.51.0. Use `text_labels` instead to retrieve string object names.\n",
      "  warnings.warn(self.message, FutureWarning)\n",
      "Processing images:  48%|████▊     | 50/104 [02:19<02:30,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/104 images. Found 165 detections, saved 164 cropped objects so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  96%|█████████▌| 100/104 [04:35<00:11,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/104 images. Found 362 detections, saved 360 cropped objects so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 104/104 [04:45<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed!\n",
      "Processed: 104 images\n",
      "Errors: 0 images\n",
      "Total detections: 373\n",
      "Images with detections: 104\n",
      "Total cropped objects saved: 371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize results storage\n",
    "all_detection_results = []\n",
    "all_cropped_info = []\n",
    "processing_stats = {\n",
    "    \"total_images\": len(image_files),\n",
    "    \"processed\": 0,\n",
    "    \"errors\": 0,\n",
    "    \"total_detections\": 0,\n",
    "    \"images_with_detections\": 0,\n",
    "    \"total_cropped_objects\": 0\n",
    "}\n",
    "\n",
    "print(f\"Starting object detection on {len(image_files)} images...\")\n",
    "print(f\"Results will be saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "# Process images with progress bar\n",
    "for i, image_path in enumerate(tqdm(image_files, desc=\"Processing images\")):\n",
    "    # Perform object detection\n",
    "    detection_result = detect_objects_in_image(image_path)\n",
    "    all_detection_results.append(detection_result)\n",
    "    \n",
    "    # Update statistics\n",
    "    processing_stats[\"processed\"] += 1\n",
    "    if \"error\" in detection_result:\n",
    "        processing_stats[\"errors\"] += 1\n",
    "    else:\n",
    "        processing_stats[\"total_detections\"] += detection_result[\"num_detections\"]\n",
    "        if detection_result[\"num_detections\"] > 0:\n",
    "            processing_stats[\"images_with_detections\"] += 1\n",
    "    \n",
    "    # Process images with detections\n",
    "    if detection_result[\"num_detections\"] > 0:\n",
    "        # Save annotated image\n",
    "        output_image_path = ANNOTATED_IMAGES_PATH / f\"annotated_{image_path.name}\"\n",
    "        annotate_and_save_image(image_path, detection_result, output_image_path)\n",
    "        \n",
    "        # Crop and save individual detected objects\n",
    "        cropped_info = crop_and_save_detections(image_path, detection_result, CROPPED_IMAGES_PATH)\n",
    "        if cropped_info:\n",
    "            all_cropped_info.extend(cropped_info)\n",
    "            processing_stats[\"total_cropped_objects\"] += len(cropped_info)\n",
    "    \n",
    "    # Print progress every 50 images\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(image_files)} images. \"\n",
    "              f\"Found {processing_stats['total_detections']} detections, \"\n",
    "              f\"saved {processing_stats['total_cropped_objects']} cropped objects so far.\")\n",
    "\n",
    "print(\"\\nProcessing completed!\")\n",
    "print(f\"Processed: {processing_stats['processed']} images\")\n",
    "print(f\"Errors: {processing_stats['errors']} images\")\n",
    "print(f\"Total detections: {processing_stats['total_detections']}\")\n",
    "print(f\"Images with detections: {processing_stats['images_with_detections']}\")\n",
    "print(f\"Total cropped objects saved: {processing_stats['total_cropped_objects']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde51b26",
   "metadata": {},
   "source": [
    "## Save Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b827b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped objects summary saved to: /home/mhieu/git/vlsp25/image/private_detected_objects/detection_results/cropped_objects_summary.csv\n",
      "Detailed results saved to: /home/mhieu/git/vlsp25/image/private_detected_objects/detection_results/detection_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save detailed results as JSON\n",
    "results_file = DETECTION_RESULTS_PATH / \"detection_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_id\": MODEL_ID,\n",
    "            \"text_prompt\": TEXT_PROMPT,\n",
    "            \"box_threshold\": BOX_THRESHOLD,\n",
    "            \"text_threshold\": TEXT_THRESHOLD,\n",
    "            \"device\": DEVICE,\n",
    "            \"statistics\": processing_stats\n",
    "        },\n",
    "        \"results\": all_detection_results,\n",
    "        \"cropped_objects\": all_cropped_info\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Save cropped objects summary as CSV\n",
    "if all_cropped_info:\n",
    "    cropped_df = pd.DataFrame(all_cropped_info)\n",
    "    cropped_summary_file = DETECTION_RESULTS_PATH / \"cropped_objects_summary.csv\"\n",
    "    cropped_df.to_csv(cropped_summary_file, index=False)\n",
    "    print(f\"Cropped objects summary saved to: {cropped_summary_file}\")\n",
    "\n",
    "print(f\"Detailed results saved to: {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlsp25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
